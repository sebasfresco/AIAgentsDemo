import json
import boto3
import botocore
import time
import logging
import math

# Initialize AWS clients for S3, Textract, and Bedrock
s3_client = boto3.client('s3')
textract_client = boto3.client('textract')
bedrock_client = boto3.client('bedrock-runtime', region_name='us-east-1')

# Set up logging
logger = logging.getLogger()
logger.setLevel(logging.INFO)

def lambda_handler(event, context):
    # Extract bucket and object key from the event
    try:
        bucket = event['Records'][0]['s3']['bucket']['name']
        key = event['Records'][0]['s3']['object']['key']
    except KeyError as e:
        logger.error("Error extracting bucket or key from event:", exc_info=True)
        return {
            'statusCode': 400,
            'body': json.dumps("Invalid event format")
        }
    
    # Skip processing if the file is a summary file
    if key.endswith('-summary.txt'):
        logger.info(f"Skipping processing for summary file: {key}")
        return {
            'statusCode': 200,
            'body': json.dumps(f"Skipped processing for summary file: {key}")
        }
    
    # Check if the document format is supported
    if not key.lower().endswith(('.pdf', '.png', '.jpeg', '.jpg', '.tiff')):
        error_message = f"Unsupported document format for {key}. Supported formats are PDF, PNG, JPEG, JPG, and TIFF."
        logger.error(error_message)
        return {
            'statusCode': 400,
            'body': json.dumps(error_message)
        }

    try:
        # Extract text from the document in S3 using AWS Textract
        text = extract_text_from_document(bucket, key)

        # Generate summary using Claude on Bedrock
        summary = generate_summary_with_claude(text)

        # Save summary back to S3
        summary_key = key.rsplit('.', 1)[0] + '-summary.txt'
        s3_client.put_object(Body=summary, Bucket=bucket, Key=summary_key)

        return {
            'statusCode': 200,
            'body': json.dumps(f'Summary saved as {summary_key}')
        }
    except Exception as e:
        logger.error(f"Error processing document: {str(e)}", exc_info=True)
        return {
            'statusCode': 500,
            'body': json.dumps(f"Error processing document: {str(e)}")
        }

def extract_text_from_document(bucket, key):
    if key.lower().endswith('.pdf'):
        # Use asynchronous Textract operation for PDFs
        job_id = start_textract_job(bucket, key)
        text = get_textract_job_results(job_id)
    else:
        # Use synchronous Textract operation for images
        response = textract_client.detect_document_text(
            Document={'S3Object': {'Bucket': bucket, 'Name': key}}
        )
        text = ''
        for block in response['Blocks']:
            if block['BlockType'] == 'LINE':
                text += block['Text'] + '\n'
        logger.info(f"Extracted text: {text[:500]}")  # Log first 500 characters for debugging
    return text

def start_textract_job(bucket, key):
    response = textract_client.start_document_text_detection(
        DocumentLocation={'S3Object': {'Bucket': bucket, 'Name': key}}
    )
    job_id = response['JobId']
    logger.info(f"Started Textract job with JobId: {job_id}")
    return job_id

def get_textract_job_results(job_id):
    # Wait for the job to complete
    max_tries = 60  # Wait up to 5 minutes (60 tries * 5 seconds)
    tries = 0
    while tries < max_tries:
        response = textract_client.get_document_text_detection(JobId=job_id)
        status = response['JobStatus']
        logger.info(f"Textract job status: {status}")
        if status == 'SUCCEEDED':
            break
        elif status == 'FAILED':
            raise Exception("Textract job failed")
        time.sleep(5)
        tries += 1
    else:
        raise Exception("Textract job did not complete in expected time.")

    # Collect all pages
    blocks = []
    next_token = None
    while True:
        if next_token:
            response = textract_client.get_document_text_detection(JobId=job_id, NextToken=next_token)
        else:
            response = textract_client.get_document_text_detection(JobId=job_id)
        blocks.extend(response.get('Blocks', []))
        next_token = response.get('NextToken')
        if not next_token:
            break

    # Extract text from all blocks
    text = ''
    for block in blocks:
        if block['BlockType'] == 'LINE':
            text += block['Text'] + '\n'

    logger.info(f"Total length of extracted text: {len(text)} characters")
    return text

def split_text_into_chunks(text, max_tokens):
    chars_per_token = 4  # Approximate characters per token
    max_chars = max_tokens * chars_per_token

    # Split the text into chunks of max_chars
    chunks = []
    text_length = len(text)
    num_chunks = math.ceil(text_length / max_chars)
    for i in range(num_chunks):
        start = i * max_chars
        end = start + max_chars
        chunks.append(text[start:end])
    return chunks

def generate_summary_with_claude(text):
    model_id = 'anthropic.claude-instant-v1'
    max_model_tokens = 9000  # Adjust based on the model's actual context window
    max_tokens_to_sample = 500

    # Define the system prompt (optional)
    system_prompt = "<<SYS>>\nYou are an academic scholar with over 25 years of experience in research, analysis, and scholarly writing. Your role is to provide clear, concise, and insightful summaries of complex texts, distilling information into key points while maintaining academic rigor and precision. You leverage your extensive knowledge and expertise to ensure that your summaries are not only accurate but also reflect a deep understanding of the subject matter.\n<</SYS>>\n\n"

    # Calculate the maximum tokens for the prompt
    prompt_tokens = len(system_prompt.split()) + 150  # Approximate tokens for the prompt and instructions
    max_tokens_per_chunk = max_model_tokens - prompt_tokens - max_tokens_to_sample

    # Split the text into chunks
    chunks = split_text_into_chunks(text, max_tokens_per_chunk)
    logger.info(f"Total chunks created: {len(chunks)}")

    summaries = []
    for idx, chunk in enumerate(chunks):
        logger.info(f"Processing chunk {idx + 1}/{len(chunks)}")

        # Build the prompt for the chunk
        prompt = f"{system_prompt}Human: Provide a comprehensive, scholarly summary of the text below. Focus on capturing the main arguments, key findings, and significant details. Ensure the summary is clear, precise, and aligned with an academic level of analysis, while retaining the original intent and nuance of the content:\n\n{chunk}\n\nAssistant:"

        try:
            response = bedrock_client.invoke_model(
                modelId=model_id,
                contentType='application/json',
                accept='application/json',
                body=json.dumps({
                    "prompt": prompt,
                    "max_tokens_to_sample": max_tokens_to_sample,
                    "temperature": 0.2,
                    "stop_sequences": ["\nHuman:"]
                })
            )

            # Extract the summary from the response
            response_body = json.loads(response['body'].read())
            logger.info(f"Bedrock response for chunk {idx + 1}: {response_body}")
            summary = response_body.get('completion', 'No summary returned')
            summaries.append(summary.strip())
        except botocore.exceptions.ClientError as e:
            if e.response['Error']['Code'] == 'ThrottlingException':
                logger.warning("Throttling exception encountered. Retrying...")
                time.sleep(2)
                return generate_summary_with_claude(text)
            else:
                logger.error("Error invoking Bedrock model:", exc_info=True)
                raise e
        except Exception as e:
            logger.error("An unexpected error occurred:", exc_info=True)
            raise e

    # Combine the summaries
    combined_summary = "\n".join(summaries)

    # Optionally, summarize the combined summary if it's too long
    if len(combined_summary.split()) > max_tokens_per_chunk:
        logger.info("Summarizing the combined summary...")
        combined_summary = summarize_summaries(combined_summary)

    return combined_summary

def summarize_summaries(summaries_text):
    model_id = 'anthropic.claude-instant-v1'
    max_tokens_to_sample = 500

    # Build the prompt to summarize the summaries
    system_prompt = "<<SYS>>\nYou are an expert at distilling multiple summaries into a concise and comprehensive overview.\n<</SYS>>\n\n"
    prompt = f"{system_prompt}Human: Please provide a consolidated summary of the following summaries:\n\n{summaries_text}\n\nAssistant:"

    try:
        response = bedrock_client.invoke_model(
            modelId=model_id,
            contentType='application/json',
            accept='application/json',
            body=json.dumps({
                "prompt": prompt,
                "max_tokens_to_sample": max_tokens_to_sample,
                "temperature": 0.2,
                "stop_sequences": ["\nHuman:"]
            })
        )

        # Extract the summary from the response
        response_body = json.loads(response['body'].read())
        logger.info(f"Bedrock response for final summary: {response_body}")
        final_summary = response_body.get('completion', 'No summary returned')
        return final_summary.strip()
    except Exception as e:
        logger.error("An error occurred while summarizing the summaries:", exc_info=True)
        raise e
